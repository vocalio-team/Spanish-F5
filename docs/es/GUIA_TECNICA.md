# Gu√≠a T√©cnica Detallada - Spanish-F5 TTS

**Versi√≥n**: 1.0
**Fecha**: 2025-10-29
**Audiencia**: Desarrolladores e ingenieros de ML

---

## üìã Tabla de Contenidos

- [Componentes Internos](#-componentes-internos)
- [Motor de Procesamiento Regional](#-motor-de-procesamiento-regional)
- [Pipeline de Inferencia](#-pipeline-de-inferencia)
- [Arquitectura de Modelos](#-arquitectura-de-modelos)
- [Procesamiento de Audio](#-procesamiento-de-audio)
- [Sistema de Prosodia](#-sistema-de-prosodia)
- [Optimizaciones CUDA](#-optimizaciones-cuda)
- [API REST Modular](#-api-rest-modular)
- [Testing y Quality Assurance](#-testing-y-quality-assurance)

---

## üîß Componentes Internos

### Estructura de Directorios

```
src/f5_tts/
‚îú‚îÄ‚îÄ core/                      # N√∫cleo del sistema
‚îÇ   ‚îú‚îÄ‚îÄ config.py             # GlobalConfig (singleton)
‚îÇ   ‚îú‚îÄ‚îÄ types.py              # AudioData, InferenceConfig, protocolos
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ
‚îú‚îÄ‚îÄ audio/                     # Procesamiento de audio
‚îÇ   ‚îú‚îÄ‚îÄ crossfading.py        # Algoritmos de crossfading
‚îÇ   ‚îú‚îÄ‚îÄ processors.py         # Pipeline de audio
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ
‚îú‚îÄ‚îÄ text/                      # Procesamiento de texto
‚îÇ   ‚îú‚îÄ‚îÄ chunker.py            # Estrategias de chunking
‚îÇ   ‚îú‚îÄ‚îÄ spanish_regional.py   # Procesador regional ‚≠ê
‚îÇ   ‚îú‚îÄ‚îÄ discourse_prosody.py  # Prosodia a nivel de discurso ‚≠ê
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ
‚îú‚îÄ‚îÄ model/                     # Definiciones de modelos
‚îÇ   ‚îú‚îÄ‚îÄ backbones/            # Arquitecturas DiT, MMDiT, UNetT
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dit.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mmdit.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ unett.py
‚îÇ   ‚îú‚îÄ‚îÄ cfm.py                # Conditional Flow Matching
‚îÇ   ‚îú‚îÄ‚îÄ dataset.py            # Manejo de datasets
‚îÇ   ‚îú‚îÄ‚îÄ trainer.py            # L√≥gica de entrenamiento
‚îÇ   ‚îî‚îÄ‚îÄ utils.py              # Utilidades de modelo
‚îÇ
‚îú‚îÄ‚îÄ infer/                     # L√≥gica de inferencia
‚îÇ   ‚îú‚îÄ‚îÄ utils_infer.py        # Utilidades principales ‚≠ê
‚îÇ   ‚îú‚îÄ‚îÄ infer_cli.py          # Interfaz CLI
‚îÇ   ‚îî‚îÄ‚îÄ infer_gradio.py       # Interfaz Gradio
‚îÇ
‚îú‚îÄ‚îÄ train/                     # Entrenamiento y finetuning
‚îÇ   ‚îú‚îÄ‚îÄ train.py
‚îÇ   ‚îú‚îÄ‚îÄ finetune_cli.py
‚îÇ   ‚îî‚îÄ‚îÄ datasets/             # Scripts de preparaci√≥n de datos
‚îÇ
‚îú‚îÄ‚îÄ rest_api/                  # REST API (REFACTORIZADO) ‚≠ê
‚îÇ   ‚îú‚îÄ‚îÄ app.py                # Factory de app FastAPI
‚îÇ   ‚îú‚îÄ‚îÄ config.py             # Configuraci√≥n de API
‚îÇ   ‚îú‚îÄ‚îÄ state.py              # Gesti√≥n de estado global
‚îÇ   ‚îú‚îÄ‚îÄ models.py             # Modelos Pydantic
‚îÇ   ‚îú‚îÄ‚îÄ enhancements.py       # Procesamiento de mejoras
‚îÇ   ‚îú‚îÄ‚îÄ tts_processor.py      # L√≥gica de generaci√≥n TTS
‚îÇ   ‚îî‚îÄ‚îÄ routes/               # Manejadores de endpoints
‚îÇ       ‚îú‚îÄ‚îÄ tts.py
‚îÇ       ‚îú‚îÄ‚îÄ upload.py
‚îÇ       ‚îú‚îÄ‚îÄ tasks.py
‚îÇ       ‚îî‚îÄ‚îÄ analysis.py
‚îÇ
‚îú‚îÄ‚îÄ api.py                     # Clase F5TTS Python API
‚îî‚îÄ‚îÄ socket_server.py           # Servidor streaming (legacy)
```

---

## üåé Motor de Procesamiento Regional

### Visi√≥n General

El procesador regional es el **componente diferenciador** de Spanish-F5, proporcionando soporte aut√©ntico para variantes regionales del espa√±ol latinoamericano.

**Archivo**: `src/f5_tts/text/spanish_regional.py` (828 l√≠neas)

### Arquitectura

```python
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         SpanishRegionalProcessor (Main Class)            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  + process_text(text, region, apply_phonetics) ‚Üí Result ‚îÇ
‚îÇ  + detect_region(text) ‚Üí RegionalVariant                ‚îÇ
‚îÇ  + normalize_text(text) ‚Üí str                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îÇ Utiliza
         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  RegionalSlang       ‚îÇ  ‚îÇ RegionalPhonetics   ‚îÇ
    ‚îÇ  ‚Ä¢ Diccionarios      ‚îÇ  ‚îÇ ‚Ä¢ Transformaciones  ‚îÇ
    ‚îÇ  ‚Ä¢ Puntuaci√≥n        ‚îÇ  ‚îÇ   fon√©ticas         ‚îÇ
    ‚îÇ  ‚Ä¢ Auto-detecci√≥n    ‚îÇ  ‚îÇ ‚Ä¢ She√≠smo/ye√≠smo    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  RegionalProsody     ‚îÇ  ‚îÇ RegionalProsodicProfile‚îÇ
    ‚îÇ  ‚Ä¢ Marcadores        ‚îÇ  ‚îÇ ‚Ä¢ Pace multipliers  ‚îÇ
    ‚îÇ  ‚Ä¢ Patrones          ‚îÇ  ‚îÇ ‚Ä¢ F0 ranges         ‚îÇ
    ‚îÇ  ‚Ä¢ Detecci√≥n         ‚îÇ  ‚îÇ ‚Ä¢ Stress patterns   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Componentes Principales

#### 1. RegionalSlang

Gestiona jerga y expresiones idiom√°ticas por regi√≥n.

```python
class RegionalSlang:
    """Diccionarios de jerga y expresiones por regi√≥n."""

    RIOPLATENSE = {
        # Interjecciones
        "che", "boludo", "pelotudo", "flaco",

        # Verbos voseo
        "quer√©s", "ten√©s", "sab√©s", "ven√≠", "and√°",

        # Sustantivos
        "colectivo", "bondi", "mate", "pilcha",

        # Expresiones
        "ni en pedo", "al pedo", "una bocha"
    }

    COLOMBIAN = {
        "parcero", "parce", "chimba", "bacano",
        "vaina", "berraco", "malparido",
        # Coletilla caracter√≠stica
        "¬øs√≠?", "¬øcierto?"
    }

    MEXICAN = {
        "√≥rale", "g√ºey", "buey", "ahorita",
        "chido", "padre", "neta",
        # Diminutivos -ito/-ita muy frecuentes
    }

    # ... otras regiones

    @staticmethod
    def detect_region(text: str) -> Tuple[str, int]:
        """
        Detecta regi√≥n basado en marcadores de jerga.

        Returns:
            (region_name, confidence_score)
        """
        scores = defaultdict(int)

        text_lower = text.lower()

        # Puntuar por cada marcador encontrado
        for region_name, markers in [
            ("rioplatense", RegionalSlang.RIOPLATENSE),
            ("colombian", RegionalSlang.COLOMBIAN),
            ("mexican", RegionalSlang.MEXICAN),
            # ...
        ]:
            for marker in markers:
                if marker in text_lower:
                    scores[region_name] += 1

        # Retornar regi√≥n con mayor puntaje
        if scores:
            best_region = max(scores.items(), key=lambda x: x[1])
            return best_region
        else:
            return ("neutral", 0)
```

**Uso:**
```python
text = "Che boludo, ¬øvos quer√©s tomar unos mates?"
region, score = RegionalSlang.detect_region(text)
# region = "rioplatense", score = 4
```

#### 2. RegionalPhonetics

Aplica transformaciones fon√©ticas caracter√≠sticas de cada regi√≥n.

```python
class RegionalPhonetics:
    """Transformaciones fon√©ticas por regi√≥n."""

    @staticmethod
    def apply_rioplatense(text: str) -> str:
        """
        Aplica transformaciones rioplatenses:
        - She√≠smo: ll/y ‚Üí sh
        - Aspiraci√≥n de /s/ final: -s ‚Üí -h
        - Voseo: Mantener acentuaci√≥n en voseo
        """
        # She√≠smo: ll/y ‚Üí sh (representado con 'sh')
        text = re.sub(r'\bll', 'sh', text)
        text = re.sub(r'\by', 'sh', text, flags=re.IGNORECASE)

        # Aspiraci√≥n de s al final de s√≠laba
        # "est√°s" ‚Üí "eht√°h"
        text = re.sub(r's\b', 'h', text)
        text = re.sub(r's([^aeiou√°√©√≠√≥√∫])', r'h\1', text)

        return text

    @staticmethod
    def apply_chilean(text: str) -> str:
        """
        Transformaciones chilenas:
        - Aspiraci√≥n/elisi√≥n de /s/
        - Reducci√≥n de /d/ intervoc√°lica
        - Entonaci√≥n caracter√≠stica
        """
        # Elisi√≥n de d intervoc√°lica
        # "todo" ‚Üí "too", "nada" ‚Üí "naa"
        text = re.sub(r'([aeiou])d([ao])\b', r'\1\2', text)

        # Aspiraci√≥n de s
        text = re.sub(r's\b', '', text)  # M√°s fuerte que rioplatense

        return text

    # ... otras regiones
```

**Ejemplo de aplicaci√≥n:**
```python
original = "¬øVos sab√©s d√≥nde est√° el colectivo?"
transformed = RegionalPhonetics.apply_rioplatense(original)
# "¬øVoh hab√©h d√≥nde eht√° el colectivo?"
# Nota: Representaci√≥n fon√©tica, no ortogr√°fica
```

#### 3. RegionalProsody

Detecta y marca patrones pros√≥dicos caracter√≠sticos.

```python
class RegionalProsody:
    """An√°lisis y marcadores de prosodia regional."""

    @staticmethod
    def analyze_rioplatense(text: str) -> List[str]:
        """
        Caracter√≠sticas pros√≥dicas rioplatenses:
        - Entonaci√≥n ascendente en preguntas
        - Doble acentuaci√≥n
        - Ritmo m√°s lento
        - Cualidad quejumbrosa
        """
        markers = []

        # Detectar preguntas (entonaci√≥n ascendente muy marcada)
        if '¬ø' in text:
            markers.append("QUESTION_RISE_STRONG")

        # Detectar exclamaciones (cualidad quejumbrosa)
        if '¬°' in text:
            markers.append("EXCLAMATION_PLAINTIVE")

        # Palabras con voseo (doble acentuaci√≥n)
        voseo_words = ["quer√©s", "sab√©s", "ten√©s", "sos", "vos"]
        for word in voseo_words:
            if word in text.lower():
                markers.append("VOSEO_STRESS")

        return markers

    @staticmethod
    def analyze_mexican(text: str) -> List[str]:
        """
        Caracter√≠sticas pros√≥dicas mexicanas:
        - Contornos mel√≥dicos distintivos
        - Entonaci√≥n expresiva
        - Diminutivos frecuentes
        """
        markers = []

        # Detectar diminutivos
        if re.search(r'\w+it[oa]s?\b', text):
            markers.append("DIMINUTIVE_AFFECTIVE")

        # Exclamaciones mexicanas (entonaci√≥n muy expresiva)
        if any(word in text.lower() for word in ["√≥rale", "h√≠jole", "√°ndale"]):
            markers.append("EXCLAMATION_EXPRESSIVE")

        return markers
```

#### 4. RegionalProsodicProfile

**Perfiles pros√≥dicos basados en investigaci√≥n acad√©mica** (NUEVO ‚≠ê)

```python
@dataclass
class RegionalProsodicProfile:
    """Perfil pros√≥dico completo validado emp√≠ricamente."""

    name: str
    pace_multiplier: float      # Multiplicador de velocidad
    f0_range_hz: Tuple[float, float]  # Rango de F0 (min, max)
    stress_pattern: str         # Patr√≥n de acentuaci√≥n
    intonation_style: str       # Estilo de entonaci√≥n
    rhythm_type: str            # Tipo de ritmo
    voice_quality: str          # Cualidad de voz

# Perfiles emp√≠ricos basados en investigaci√≥n
RIOPLATENSE_PROFILE = RegionalProsodicProfile(
    name="rioplatense",
    pace_multiplier=0.75,        # LENTO (Cuello & Oro Oz√°n 2024)
    f0_range_hz=(75, 340),       # Femenino: 75-340Hz
    stress_pattern="double_accentuation",  # Doble acentuaci√≥n
    intonation_style="circumflex",  # Circunfleja
    rhythm_type="stress_timed",
    voice_quality="plaintive"    # Cualidad quejumbrosa
)

COLOMBIAN_PROFILE = RegionalProsodicProfile(
    name="colombian",
    pace_multiplier=1.0,         # MEDIO
    f0_range_hz=(80, 280),
    stress_pattern="clear",
    intonation_style="neutral",
    rhythm_type="syllable_timed",
    voice_quality="clear"
)

MEXICAN_PROFILE = RegionalProsodicProfile(
    name="mexican",
    pace_multiplier=1.0,         # MEDIO
    f0_range_hz=(85, 320),
    stress_pattern="melodic",
    intonation_style="expressive",
    rhythm_type="mixed",
    voice_quality="warm"
)
```

**Referencias Acad√©micas:**
- **Cuello & Oro Oz√°n (2024)**: "An√°lisis de la prosodia del espa√±ol rioplatense a trav√©s del corpus T.E.P.HA"
- **Guglielmone et al. (2014)**: "La prosodia del espa√±ol rioplatense en el marco de la teor√≠a de la optimalidad"

### Flujo de Procesamiento Completo

```python
def process_spanish_text(
    text: str,
    region: str = "auto",
    apply_phonetics: bool = True
) -> ProcessingResult:
    """
    Pipeline completo de procesamiento regional.

    Steps:
    1. Normalizar texto b√°sico
    2. Detectar/aplicar jerga regional
    3. Aplicar transformaciones fon√©ticas (opcional)
    4. Agregar marcadores pros√≥dicos
    5. Aplicar perfil pros√≥dico emp√≠rico
    6. Retornar resultado estructurado
    """
    processor = SpanishRegionalProcessor()

    # 1. Normalizaci√≥n b√°sica
    normalized = processor.normalize_text(text)

    # 2. Detecci√≥n de regi√≥n (si auto)
    if region == "auto":
        detected_region, confidence = processor.detect_region(text)
    else:
        detected_region = region

    # 3. Aplicar transformaciones fon√©ticas
    if apply_phonetics:
        phonetic = processor.apply_phonetics(normalized, detected_region)
    else:
        phonetic = normalized

    # 4. Agregar marcadores pros√≥dicos
    prosody_markers = processor.analyze_prosody(text, detected_region)

    # 5. Obtener perfil pros√≥dico emp√≠rico
    prosodic_profile = processor.get_prosodic_profile(detected_region)

    # 6. Retornar resultado completo
    return ProcessingResult(
        original_text=text,
        normalized_text=normalized,
        phonetic_text=phonetic,
        detected_region=detected_region,
        slang_markers=processor.get_slang_markers(text, detected_region),
        prosody_markers=prosody_markers,
        prosodic_profile=prosodic_profile
    )
```

**Ejemplo de uso completo:**
```python
from f5_tts.text import process_spanish_text

result = process_spanish_text(
    "Che boludo, ¬øvos quer√©s tomar unos mates?",
    region="auto",  # Detectar√° "rioplatense"
    apply_phonetics=True
)

print(f"Regi√≥n detectada: {result.detected_region}")
# "rioplatense"

print(f"Marcadores de jerga: {result.slang_markers}")
# ["che", "boludo", "vos", "quer√©s"]

print(f"Perfil pros√≥dico: pace={result.prosodic_profile.pace_multiplier}x")
# pace=0.75x (lento, emp√≠ricamente validado)

print(f"Rango F0: {result.prosodic_profile.f0_range_hz} Hz")
# (75, 340) Hz para voz femenina
```

---

## üéØ Pipeline de Inferencia

### Visi√≥n General del Flujo

```
Input Text
    ‚îÇ
    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Text Preprocessing  ‚îÇ ‚Üê Regional processing
‚îÇ - Normalize         ‚îÇ
‚îÇ - Phonetics         ‚îÇ
‚îÇ - Prosody markers   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Text Encoding       ‚îÇ
‚îÇ - Tokenization      ‚îÇ
‚îÇ - Embedding         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Reference Audio     ‚îÇ
‚îÇ - Load ref audio    ‚îÇ
‚îÇ - Extract mel-spec  ‚îÇ
‚îÇ - Encode features   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ DiT/UNetT Model     ‚îÇ ‚Üê GPU Inference
‚îÇ - Flow matching     ‚îÇ
‚îÇ - NFE steps (8-64)  ‚îÇ
‚îÇ - Generate mel-spec ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Vocoder (Vocos)     ‚îÇ
‚îÇ - Mel ‚Üí Waveform    ‚îÇ
‚îÇ - 24kHz output      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Post-Processing     ‚îÇ
‚îÇ - Normalize         ‚îÇ
‚îÇ - Crossfade         ‚îÇ
‚îÇ - Quality check     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
   Output WAV
```

### C√≥digo de Inferencia

**Archivo**: `src/f5_tts/infer/utils_infer.py`

```python
def infer_batch(
    model,
    vocoder,
    ref_audio,
    ref_text,
    gen_text_batches,
    cfg_strength=2.0,
    nfe_step=32,
    speed=1.0,
    sway_sampling_coef=-1.0,
    device="cuda"
):
    """
    Inferencia por lotes con DiT/UNetT.

    Args:
        model: Modelo DiT o UNetT
        vocoder: Vocos vocoder
        ref_audio: Audio de referencia (tensor)
        ref_text: Texto de referencia
        gen_text_batches: Lista de textos a generar
        cfg_strength: Classifier-free guidance (1.5-3.0)
        nfe_step: N√∫mero de pasos de integraci√≥n (8-64)
        speed: Multiplicador de velocidad
        sway_sampling_coef: Coeficiente de sway sampling
        device: cuda/cpu

    Returns:
        generated_waves: Lista de waveforms generados
    """
    # 1. Preparar audio de referencia
    ref_audio = ref_audio.to(device)

    # 2. Extraer mel-espectrograma de referencia
    with torch.no_grad():
        ref_mel = vocoder.extract_mel(ref_audio)

    # 3. Tokenizar textos
    ref_text_tokens = tokenize(ref_text)
    gen_text_tokens = [tokenize(text) for text in gen_text_batches]

    # 4. Generar mel-espectrogramas con modelo DiT
    with torch.no_grad():
        generated_mels = model.sample(
            cond=ref_mel,
            text=gen_text_tokens,
            duration=None,  # Auto-detect
            steps=nfe_step,
            cfg_strength=cfg_strength,
            sway_sampling_coef=sway_sampling_coef
        )

    # 5. Vocoder: mel ‚Üí waveform
    generated_waves = []
    for mel in generated_mels:
        with torch.no_grad():
            wave = vocoder.decode(mel)

        # Aplicar speed
        if speed != 1.0:
            wave = resample_audio(wave, speed)

        generated_waves.append(wave)

    return generated_waves
```

### Par√°metros de Calidad

| Par√°metro | Rango | Default | Efecto |
|-----------|-------|---------|--------|
| `nfe_step` | 8-64 | 32 | Pasos de integraci√≥n (‚Üë = mejor calidad, ‚Üì = m√°s r√°pido) |
| `cfg_strength` | 1.0-5.0 | 2.0 | Guidance strength (‚Üë = m√°s fiel a condici√≥n) |
| `speed` | 0.5-2.0 | 1.0 | Velocidad de habla |
| `sway_sampling_coef` | -1 a 1 | -1 | Coeficiente de sway (-1 = auto) |

**Recomendaciones:**
- **Calidad alta**: nfe=32-64, cfg=2.0-2.5
- **Balanceado**: nfe=16-24, cfg=2.0
- **R√°pido**: nfe=8-12, cfg=1.8-2.0

---

## üèóÔ∏è Arquitectura de Modelos

### DiT (Diffusion Transformer)

**Archivo**: `src/f5_tts/model/backbones/dit.py`

```python
class DiT(nn.Module):
    """
    Diffusion Transformer para F5-TTS.

    Arquitectura:
    - Transformer encoder-decoder
    - Posicional embeddings rotatorios
    - Multi-head attention con flash attention
    - Feed-forward con SwiGLU activation
    """

    def __init__(
        self,
        dim=1024,
        depth=24,
        heads=16,
        ff_mult=4,
        text_dim=512,
        conv_layers=4
    ):
        super().__init__()

        # Text encoder
        self.text_embed = nn.Embedding(vocab_size, text_dim)

        # Time embedding (difusi√≥n)
        self.time_embed = TimestepEmbedding(dim)

        # Transformer layers
        self.layers = nn.ModuleList([
            TransformerBlock(
                dim=dim,
                heads=heads,
                ff_mult=ff_mult
            )
            for _ in range(depth)
        ])

        # Output projection
        self.to_pred = nn.Linear(dim, mel_dim)

    def forward(self, x, cond, text, time):
        """
        Args:
            x: Noisy mel-spectrogram [B, T, mel_dim]
            cond: Conditioning mel [B, T_ref, mel_dim]
            text: Text tokens [B, T_text]
            time: Diffusion timestep [B]

        Returns:
            pred: Predicted noise [B, T, mel_dim]
        """
        # Embed text
        text_emb = self.text_embed(text)  # [B, T_text, text_dim]

        # Time embedding
        t_emb = self.time_embed(time)  # [B, dim]

        # Concatenate conditioning
        x = torch.cat([cond, x], dim=1)  # [B, T_ref+T, mel_dim]

        # Add time embedding
        x = x + t_emb[:, None, :]

        # Transformer layers con cross-attention a texto
        for layer in self.layers:
            x = layer(x, context=text_emb)

        # Output
        pred = self.to_pred(x[:, cond.size(1):, :])  # Solo parte generada

        return pred
```

### Conditional Flow Matching (CFM)

**Archivo**: `src/f5_tts/model/cfm.py`

Flow matching es una alternativa a difusi√≥n est√°ndar que ofrece:
- Convergencia m√°s r√°pida
- Mejor calidad de audio
- Menor n√∫mero de pasos de inferencia

```python
class ConditionalFlowMatching:
    """
    Conditional Flow Matching para generaci√≥n de mel-espectrogramas.

    Usa ODE (Ordinary Differential Equation) solver para muestreo.
    """

    def __init__(self, model, sigma_min=1e-4):
        self.model = model
        self.sigma_min = sigma_min

    def sample(
        self,
        cond,
        text,
        steps=32,
        cfg_strength=2.0,
        method="euler"
    ):
        """
        Muestreo con flow matching.

        Args:
            cond: Conditioning (ref mel-spec)
            text: Text tokens
            steps: Integration steps (NFE)
            cfg_strength: Classifier-free guidance
            method: ODE solver (euler, heun, rk4)

        Returns:
            mel: Generated mel-spectrogram
        """
        # 1. Inicializar con ruido
        B, T = text.shape[0], self.estimate_duration(text)
        z = torch.randn(B, T, self.mel_dim).to(cond.device)

        # 2. Timestepping
        dt = 1.0 / steps
        t_schedule = torch.linspace(0, 1, steps+1)

        # 3. Integrar ODE
        for i in range(steps):
            t = t_schedule[i]

            # Predecir velocidad
            with torch.no_grad():
                v_pred = self.model(z, cond, text, t)

                # Classifier-free guidance
                if cfg_strength != 1.0:
                    v_uncond = self.model(z, cond, None, t)
                    v_pred = v_uncond + cfg_strength * (v_pred - v_uncond)

            # Euler step
            z = z + v_pred * dt

        return z
```

### Vocoder (Vocos)

**Vocos** es un vocoder basado en convoluciones que convierte mel-espectrogramas a waveforms.

**Caracter√≠sticas:**
- Arquitectura ligera (~5M par√°metros)
- Inferencia r√°pida (GPU: ~10ms para 5s audio)
- Alta calidad (comparable a HiFi-GAN)

```python
class Vocos:
    """Wrapper para vocoder Vocos."""

    def __init__(self, checkpoint_path, device="cuda"):
        self.model = load_vocos_model(checkpoint_path)
        self.model.to(device)
        self.model.eval()

    def decode(self, mel):
        """
        Mel-spectrogram ‚Üí Waveform

        Args:
            mel: [B, mel_bins, T] o [mel_bins, T]

        Returns:
            wave: [B, samples] o [samples]
        """
        with torch.no_grad():
            wave = self.model.decode(mel)
        return wave

    def extract_mel(self, audio, sr=24000):
        """
        Waveform ‚Üí Mel-spectrogram

        Args:
            audio: [samples] o [B, samples]
            sr: Sample rate

        Returns:
            mel: [mel_bins, T] o [B, mel_bins, T]
        """
        with torch.no_grad():
            mel = self.model.extract_mel(audio, sr)
        return mel
```

---

## üéµ Procesamiento de Audio

### Crossfading

**Archivo**: `src/f5_tts/audio/crossfading.py`

Tres algoritmos implementados:

#### 1. Equal Power Crossfading (Est√°ndar Industria)

```python
class EqualPowerCrossfader:
    """
    Crossfading con ley de potencia igual.

    Previene ca√≠das de amplitud durante transiciones.
    Usa curvas de fade sqrt para mantener potencia constante.
    """

    def crossfade(
        self,
        audio1: np.ndarray,
        audio2: np.ndarray,
        duration: float = 0.5,
        sample_rate: int = 24000
    ) -> np.ndarray:
        """
        Aplica equal-power crossfade.

        Args:
            audio1: Primer segmento de audio
            audio2: Segundo segmento de audio
            duration: Duraci√≥n del fade (segundos)
            sample_rate: Sample rate

        Returns:
            Concatenated audio con crossfade suave
        """
        fade_samples = int(duration * sample_rate)

        # Curvas de fade (sqrt para equal power)
        fade_out = np.sqrt(np.linspace(1, 0, fade_samples))
        fade_in = np.sqrt(np.linspace(0, 1, fade_samples))

        # Aplicar fades
        audio1_end = audio1[-fade_samples:]
        audio2_start = audio2[:fade_samples]

        crossfaded = audio1_end * fade_out + audio2_start * fade_in

        # Concatenar
        result = np.concatenate([
            audio1[:-fade_samples],
            crossfaded,
            audio2[fade_samples:]
        ])

        return result
```

#### 2. Raised Cosine Crossfading

```python
class RaisedCosineCrossfader:
    """
    Crossfading con ventana raised cosine (Hann).
    Transiciones a√∫n m√°s suaves que equal power.
    """

    def crossfade(self, audio1, audio2, duration=0.5, sample_rate=24000):
        fade_samples = int(duration * sample_rate)

        # Ventana Hann
        fade_out = np.cos(np.linspace(0, np.pi/2, fade_samples)) ** 2
        fade_in = np.sin(np.linspace(0, np.pi/2, fade_samples)) ** 2

        # ... resto igual que equal power
```

#### 3. Linear Crossfading

```python
class LinearCrossfader:
    """Crossfading lineal simple (menos smooth)."""

    def crossfade(self, audio1, audio2, duration=0.5, sample_rate=24000):
        fade_samples = int(duration * sample_rate)

        fade_out = np.linspace(1, 0, fade_samples)
        fade_in = np.linspace(0, 1, fade_samples)

        # ... resto igual
```

**Recomendaci√≥n**: Usar **EqualPowerCrossfader** para mejor calidad.

### Audio Processing Pipeline

```python
from f5_tts.audio import (
    remove_dc_offset,
    normalize_rms,
    resample_audio,
    prevent_clipping,
    AudioProcessingPipeline
)

# Pipeline completo
pipeline = AudioProcessingPipeline([
    remove_dc_offset,      # 1. Remover DC offset
    normalize_rms,         # 2. Normalizar RMS
    resample_audio,        # 3. Resamplear a 24kHz
    prevent_clipping       # 4. Prevenir clipping
])

# Aplicar pipeline
processed = pipeline.process(raw_audio, target_sr=24000, target_rms=0.1)
```

### Audio Quality Analyzer

```python
from f5_tts.audio import AudioQualityAnalyzer

analyzer = AudioQualityAnalyzer()
quality = analyzer.analyze("output.wav")

print(f"Overall Quality: {quality.overall_level}")  # EXCELLENT/GOOD/FAIR/POOR
print(f"SNR: {quality.snr_db:.1f} dB")
print(f"Clipping Rate: {quality.clipping_rate:.2%}")
print(f"Dynamic Range: {quality.dynamic_range_db:.1f} dB")
print(f"Issues: {quality.detected_issues}")
```

---

## üìä Sistema de Prosodia

### Prosodia a Nivel de Discurso

**Archivo**: `src/f5_tts/text/discourse_prosody.py`

Basado en **Guglielmone et al. (2014)**, implementa:

- **Unidades de declinaci√≥n**: Secciones tem√°ticas con reset de F0
- **Tonos nucleares**: Descendente (‚Üò), suspensivo (‚Üí), ascendente (‚Üó)
- **Frases entonacionales**: Unidades pros√≥dicas completas

```python
from f5_tts.text.discourse_prosody import analyze_discourse_prosody

result = analyze_discourse_prosody(
    "Hola amigo. ¬øC√≥mo est√°s? Estoy muy bien, gracias.",
    voice_type="female"
)

# result.phrases:
# [
#   IntonationalPhrase(text="Hola amigo", nuclear_tone="descending"),
#   IntonationalPhrase(text="¬øC√≥mo est√°s?", nuclear_tone="ascending"),
#   IntonationalPhrase(text="Estoy muy bien, gracias", nuclear_tone="descending")
# ]

# result.declination_units:
# [
#   DeclinationUnit(
#     phrases=[...],
#     f0_start=200,
#     f0_end=120
#   )
# ]
```

---

## ‚ö° Optimizaciones CUDA

### torch.compile

```python
import torch

# Habilitar compilaci√≥n de modelo
model = torch.compile(
    model,
    mode="reduce-overhead",  # o "max-autotune"
    backend="inductor"
)

# Beneficios:
# - 20-40% reducci√≥n de latencia
# - Kernels fusionados
# - Menos overhead de Python
```

### cuDNN Benchmark

```python
import torch

# Auto-tuning de kernels cuDNN
torch.backends.cudnn.benchmark = True

# Beneficios:
# - Selecciona kernels √≥ptimos por tama√±o de input
# - 5-10% ganancia de velocidad
```

### TF32 (Tensor Float 32)

```python
# GPUs Ampere+ (A100, RTX 3090, etc.)
torch.set_float32_matmul_precision("high")  # Habilita TF32

# Beneficios:
# - 8x m√°s r√°pido que FP32
# - Misma precisi√≥n para deep learning
```

### Variables de Entorno

```bash
# Configurar optimizaciones
export ENABLE_TORCH_COMPILE=true
export ENABLE_CUDNN_BENCHMARK=true
export TORCH_MATMUL_PRECISION=high

# Ejecutar API
python f5_tts_api.py
```

---

## üåê API REST Modular

### Refactorizaci√≥n

La API fue refactorizada de un monolito de **1000+ l√≠neas** a una arquitectura modular:

```
f5_tts_api.py (48 l√≠neas) ‚Üí Entry point
    ‚îÇ
    ‚îî‚îÄ‚Üí src/f5_tts/rest_api/
        ‚îú‚îÄ‚îÄ app.py (180 l√≠neas) - Factory de app
        ‚îú‚îÄ‚îÄ state.py (120 l√≠neas) - Estado global
        ‚îú‚îÄ‚îÄ models.py (200 l√≠neas) - Modelos Pydantic
        ‚îú‚îÄ‚îÄ enhancements.py (150 l√≠neas) - Mejoras de texto
        ‚îú‚îÄ‚îÄ tts_processor.py (180 l√≠neas) - L√≥gica TTS
        ‚îî‚îÄ‚îÄ routes/ - Endpoints organizados
            ‚îú‚îÄ‚îÄ tts.py (140 l√≠neas)
            ‚îú‚îÄ‚îÄ upload.py (90 l√≠neas)
            ‚îú‚îÄ‚îÄ tasks.py (70 l√≠neas)
            ‚îî‚îÄ‚îÄ analysis.py (60 l√≠neas)
```

**Beneficios:**
- ‚úÖ Modularidad y testabilidad
- ‚úÖ Separaci√≥n de responsabilidades
- ‚úÖ Reutilizaci√≥n de c√≥digo
- ‚úÖ 100% compatibilidad hacia atr√°s

Ver [docs/API_REFACTORING.md](../API_REFACTORING.md) para detalles completos.

---

## üß™ Testing y Quality Assurance

### Cobertura de Tests

**287 tests** con **60% cobertura de c√≥digo**

#### Test Suites

1. **Regional Spanish** (`test_spanish_regional.py`)
   - 40 tests
   - Transformaciones fon√©ticas por regi√≥n
   - Detecci√≥n autom√°tica de variantes
   - Normalizaci√≥n de jerga

2. **Prosody Improvements** (`test_prosody_improvements.py`)
   - 31 tests ‚≠ê
   - Perfiles pros√≥dicos emp√≠ricos
   - Prosodia a nivel de discurso
   - Prevenci√≥n de regresiones

3. **Audio Processing** (`test_audio_processors.py`, `test_audio.py`)
   - 50 tests
   - Algoritmos de crossfading
   - Normalizaci√≥n de audio
   - An√°lisis de calidad

4. **API Integration** (`test_api_integration.py`)
   - 31 tests
   - Endpoints REST
   - Gesti√≥n de estado
   - Procesamiento de mejoras

### Ejecutar Tests

```bash
# Suite completa
pytest tests/ -v

# Con cobertura
pytest --cov=src/f5_tts --cov-report=html tests/

# Suite espec√≠fica
pytest tests/test_prosody_improvements.py -v

# Test espec√≠fico
pytest tests/test_spanish_regional.py::test_rioplatense_phonetics -v
```

### An√°lisis de Cobertura

```bash
python analyze_coverage.py

# Genera:
# - htmlcov/ - Reporte HTML interactivo
# - Resumen por m√≥dulo
# - L√≠neas no cubiertas
```

---

## üìö Referencias

### Papers Acad√©micos

- **F5-TTS**: [arXiv:2410.06885](https://arxiv.org/abs/2410.06885)
- **E2-TTS**: [arXiv:2406.18009](https://arxiv.org/abs/2406.18009)
- **Cuello & Oro Oz√°n (2024)**: Prosodia T.E.P.HA rioplatense
- **Guglielmone et al. (2014)**: Prosodia del espa√±ol rioplatense

### Documentaci√≥n Relacionada

- **[README.es.md](../../README.es.md)** - Documentaci√≥n principal
- **[ARQUITECTURA_ECOSISTEMA.md](ARQUITECTURA_ECOSISTEMA.md)** - Integraci√≥n Vocalio
- **[../ARCHITECTURE.md](../ARCHITECTURE.md)** - Arquitectura interna (ingl√©s)
- **[../PROSODY_ANALYSIS_ACADEMIC_PAPERS.md](../PROSODY_ANALYSIS_ACADEMIC_PAPERS.md)** - An√°lisis acad√©mico

---

<div align="center">

**Documentaci√≥n T√©cnica - Spanish-F5 TTS**

**Parte del Ecosistema Vocalio**

</div>
